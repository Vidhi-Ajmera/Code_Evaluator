<!-- ---

# 🚀 Code Evaluator  

🔍 **AI-Based Code Evaluator** - A powerful AI-powered system designed for automated code evaluation, enhancing efficiency in technical assessments and coding competitions.  

## 📌 Project Overview  

The **Code Evaluator** is an intelligent system that allows evaluators to create coding contests, assess code submissions, and provide real-time feedback. The system supports multiple programming languages and automates the grading process using AI-based techniques.  

This project is ideal for:  
✔️ **Technical Evaluation** - Automate candidate code evaluations.  
✔️ **Contests** - Conduct coding competitions with instant grading.  
✔️ **Education** - Help students learn programming with AI-driven feedback.  

---

## 🌐 Live Project Link  

🔗 **[Project Demo](https://code-evaluator-frontend.vercel.app/)**  

---

## ✨ Features  

✅ **Multi-Language Support** - Supports Python, Java, C++, and more.  
✅ **Automated Code Evaluation** - Executes code with predefined test cases.  
✅ **Real-Time Feedback** - Provides immediate results and analysis.  
✅ **AI-Powered Proctoring** - Monitors participants during contests.  
✅ **Plagiarism Detection** - Ensures the originality of code submissions.  
✅ **Contest Management** - Allows evaluators to create and manage coding contests.  
✅ **Interactive Dashboard** - Displays participant performance and rankings.  

---

## 🛠 Installation & Setup  

### 1️⃣ Clone the Repository  

```bash
git clone https://github.com/Vidhi-Ajmera/Code_Evaluator.git
cd Code_Evaluator
```

### 2️⃣ Install Dependencies  

#### 📌 Backend Setup (Python)  

```bash
cd backend
pip install -r requirements.txt
python main.py
```

#### 📌 Frontend Setup (React.js)  

```bash
cd frontend
npm install
npm start
```

---

## 🔧 Usage  

### 🎯 **For Evaluators**  
1️⃣ **Login** as an evaluator.  
2️⃣ **Create** a new coding contest by specifying the number of questions.  
3️⃣ **Define** problem statements, test cases, and scoring criteria.  
4️⃣ **Share** the contest link with participants.  
5️⃣ **Monitor submissions**, view results, and analyze participant performance.  

### 🎯 **For Participants**  
1️⃣ **Sign up** and join an active contest.  
2️⃣ **Select** a problem statement and write your code.  
3️⃣ **Submit** your solution for evaluation.  
4️⃣ **Receive feedback**, including test case results and scores.  
5️⃣ **Check the leaderboard** to see your ranking.  

---

## 📂 Project Structure  

```
Code_Evaluator/
│── backend/        # AI-based evaluation engine (Flask/FastAPI)
│   ├── models/     # ML models for code evaluation
│   ├── routes/     # API endpoints
│   ├── tests/      # Backend unit tests
│   ├── main.py     # Backend entry point
│
│── frontend/       # React-based web application
│   ├── src/        # React components and pages
│   ├── public/     # Static assets
│   ├── package.json # Frontend dependencies
│
│── docs/           # Documentation and API specifications
│── tests/          # Integration tests
│── .gitignore      # Git ignore file
│── README.md       # Project documentation
```

---

## 🚀 Future Enhancements  
🔹 **AI-powered hints and recommendations** for incorrect code submissions.  
🔹 **Enhanced plagiarism detection** using deep learning models.  
🔹 **Live coding environment** for real-time collaboration and debugging.  

---

## 👥 Contributors  

- **Vidhi Ajmera**  
- **Anshika Tripathi**  
- **Deepak Shukla**  
- **Ekta Agrawal**  
- **Saubhagya Sharma**  


--- -->

# 🚀 Code Evaluator  

🔍 **AI-Powered Code Evaluation & Learning Platform**  
An intelligent system that auto-detects programming languages, evaluates code for correctness and plagiarism, and provides smart AI-generated feedback. Whether you're a **teacher**, **student**, or a **developer**, this platform bridges the gap between coding practice and real-time evaluation – all in one place.

---

## 📌 Project Overview  

### 🧠 Two Powerful Modules in One Platform:

#### 🔹 **1. Code Evaluation Tool (Live Now)**
A quick and efficient space where users can **paste or write code**, and the platform will:
- **Auto-detect the programming language** – no manual selection needed.
- Offer **two visual themes** – Light ☀️ and Dark 🌙 – for better readability.
- On clicking **“Analyze Code”**, generate a real-time **AI-powered report** including:
  - ✅ **Correctness** – Did the code pass the test cases?
  - 📛 **Plagiarism Detection** – Check if the code is copied.
  - 🌍 **Source Tracing** – Show where the code might be copied from.
  - 💡 **Improvement Suggestions** – Learn how to write better code.
  - 🧮 **Scoring System** – Get a numerical score for each submission.

#### 🔹 **2. Coding Platform for Educators & Learners (Future Scope)**
A dedicated portal for structured learning and evaluation:
- 👨‍🏫 **Teachers** can upload problems, assign them to students, and track performance.
- 🧑‍🎓 **Students** can solve problems based on topics/difficulty and receive instant AI feedback.

---

## 🌐 Live Project  

🔗 **[Try the Live Demo](https://code-evaluator-frontend.vercel.app/)** – Currently hosts the Code Evaluation Tool.  

---

## ✨ Core Features  

### 🔹 **Code Evaluation Tool (Available Now)**  
- ✍️ Paste or write code with **auto language detection**.  
- 🎨 Switch between **light/dark themes**.  
- ⚡ Click **“Analyze Code”** to instantly get an **AI-generated evaluation report** with:
  - ✅ Test case result (pass/fail)
  - 🔍 Plagiarism percentage and source
  - 💬 Suggestions for improvement
  - 🧠 Final score and report insights

---

### 🔹 **Coding Platform for Educators & Learners (In Progress)**  

#### 🧑‍🏫 **For Teachers / Evaluators**
- Upload coding problems based on topics and levels.  
- Track and evaluate student submissions per question.  
- View insights on student engagement and problem difficulty.  
- Use the **AI engine** to assess correctness, plagiarism, and offer feedback automatically.

#### 🧑‍🎓 **For Students / Participants**
- Role-based login to access personalized dashboards.  
- Solve problems from various topics and difficulty levels.  
- Auto language detection while coding.  
- Get instant AI feedback after each submission.  
- Track your journey with **streaks, participation history**, and a **gamified leaderboard**.

---

## 🛠 Installation & Setup  

### 1️⃣ Clone the Repository  
```bash
git clone https://github.com/Vidhi-Ajmera/Code_Evaluator.git
cd Code_Evaluator
```

### 2️⃣ Backend Setup (Python + FastAPI)  
```bash
cd backend
pip install -r requirements.txt
python main.py
```

### 3️⃣ Frontend Setup (React.js)  
```bash
cd frontend
npm install
npm start
```

---

## 📂 Project Structure  

```
Code_Evaluator/
│
├── backend/              # AI evaluation engine (FastAPI)
│   ├── models/           # ML models for correctness & plagiarism
│   ├── routes/           # API endpoints
│   ├── main.py           # Backend entry point
│
├── frontend/             # React-based frontend
│   ├── src/              # Components and pages
│   ├── public/           # Static assets (icons, images, etc.)
│
├── docs/                 # Documentation and specifications
├── tests/                # Integration/unit tests
├── README.md             # Project overview
└── .gitignore            # Ignored files for version control
```

---

## 🔮 Planned Enhancements  

✨ Future versions will include:  
- **AI-Powered Coding Hints** – Personalized guidance on logic or structure.  
- **Real-Time Collaboration** – Pair programming and live contest features.  
- **Gamified Dashboards** – Visual performance tracking, badges, and XP.  
- **Voice Narration & Accessibility** – For inclusive coding experiences.  
- **Integrated Code Editor with Auto-Save** – Boosting the development experience.

---

## 👥 Contributors  

- 👩‍💻 **Vidhi Ajmera**   
- 👨‍💻 **Deepak Shukla**  
- 👩‍💻 **Ekta Agrawal** 
- 👩‍💻 **Anshika Tripathi** 
- 👨‍💻 **Saubhagya Sharma**  

